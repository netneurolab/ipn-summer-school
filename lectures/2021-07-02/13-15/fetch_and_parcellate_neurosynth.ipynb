{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fetch_and_parcellate_neurosynth.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCx8_PJhUluy"
      },
      "source": [
        "Script for fetching and parcellating Neurosynth meta-analyses (association tests) for 123 cognitive atlas terms.\n",
        "Terms will be parcellated according to the 200-node Schaefer 2018 7-network parcellation.\n",
        "This notebook will take approximately 25 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM8Sv2Y3URa2"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Script for performing NeuroSynth-style meta-analyses for all available\n",
        "Cognitive Atlas concepts. Script modified from https://github.com/netneurolab/\n",
        "markello_spatialnulls/blob/master/scripts/empirical/fetch_neurosynth_maps.py,\n",
        "for the IPN Summer School > Quantitative and Computational Neuroscience >\n",
        "Advanced Analytics for Neuroscience > Contextualizing Results lecture.\n",
        "\n",
        "Again, this is mostly Ross' code.\n",
        "\"\"\"\n",
        "\n",
        "!pip install nilearn\n",
        "!pip install --upgrade numpy scipy matplotlib pandas\n",
        "!pip install git+https://github.com/neurosynth/neurosynth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZgYGMEU1Nr"
      },
      "source": [
        "Import stuff.\n",
        "If you run into errors, check if you need to `!pip install --upgrade` or `pip install`. \n",
        "(You shouldn't, but who knows.)\n",
        "\n",
        "Just in case, let's check that your `pandas` is up to date (`version == 1.2.5`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c-XzsFs41E7",
        "outputId": "1f799aa7-afec-4e41-a33e-28b24ff5f8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# check pandas version because shrug\n",
        "import pandas as pd\n",
        "pd.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3FWTlVRUVOz",
        "outputId": "44b722e6-bbca-40f6-a6d2-5b3004d48e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import contextlib\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "from nilearn.input_data import NiftiLabelsMasker\n",
        "from nilearn._utils import check_niimg\n",
        "\n",
        "import neurosynth as ns\n",
        "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
        "from google.colab import files"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
            "  \"Numpy arrays.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnBInV2AVIV4"
      },
      "source": [
        "Some directory set-up and warning ignoring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS3KA1CuUaPM"
      },
      "source": [
        "# /sigh\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "# this is where the raw and parcellated data will be stored\n",
        "NSDIR = Path('./data/raw/neurosynth').resolve()\n",
        "PARDIR = Path('./data/derivatives/neurosynth').resolve()\n",
        "\n",
        "# these are the images from the neurosynth analyses we'll save\n",
        "# can add 'uniformity-test_z' plus more, if desired\n",
        "IMAGES = ['association-test_z']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9e8KUXLVOU7"
      },
      "source": [
        "All the functions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ukJcuJ2UeFZ"
      },
      "source": [
        "def fetch_ns_data(directory):\n",
        "    \"\"\" Fetches NeuroSynth database + features to `directory`\n",
        "    Paramerters\n",
        "    -----------\n",
        "    directory : str or os.PathLike\n",
        "        Path to directory where data should be saved\n",
        "    Returns\n",
        "    -------\n",
        "    database, features : PathLike\n",
        "        Paths to downloaded NS data\n",
        "    \"\"\"\n",
        "\n",
        "    directory = Path(directory)\n",
        "\n",
        "    # if not already downloaded, download the NS data and unpack it\n",
        "    database, features = directory / 'database.txt', directory / 'features.txt'\n",
        "    if not database.exists() or not features.exists():\n",
        "        with open(os.devnull, 'w') as f, contextlib.redirect_stdout(f):\n",
        "            ns.dataset.download(path=directory, unpack=True)\n",
        "        try:  # remove tarball if it wasn't removed for some reason\n",
        "            (directory / 'current_data.tar.gz').unlink()\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    return database, features\n",
        "\n",
        "\n",
        "def get_cogatlas_concepts(url=None):\n",
        "    \"\"\" Fetches list of concepts from the Cognitive Atlas\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        URL to Cognitive Atlas API\n",
        "    Returns\n",
        "    -------\n",
        "    concepts : set\n",
        "        Unordered set of terms\n",
        "    \"\"\"\n",
        "\n",
        "    if url is None:\n",
        "        url = 'https://cognitiveatlas.org/api/v-alpha/concept'\n",
        "\n",
        "    req = requests.get(url)\n",
        "    req.raise_for_status()\n",
        "    concepts = set([f.get('name') for f in json.loads(req.content)])\n",
        "\n",
        "    return concepts\n",
        "\n",
        "\n",
        "def run_meta_analyses(database, features, use_features=None, outdir=None):\n",
        "    \"\"\"\n",
        "    Runs NS-style meta-analysis based on `database` and `features`\n",
        "    Parameters\n",
        "    ----------\n",
        "    database, features : str or os.PathLike\n",
        "        Path to NS-style database.txt and features.txt files\n",
        "    use_features : list, optional\n",
        "        List of features on which to run NS meta-analyses; if not supplied all\n",
        "        terms in `features` will be used\n",
        "    outdir : str or os.PathLike\n",
        "        Path to output directory where derived files should be saved\n",
        "    Returns\n",
        "    -------\n",
        "    generated : list of str\n",
        "        List of filepaths to generated term meta-analysis directories\n",
        "    \"\"\"\n",
        "\n",
        "    # check outdir\n",
        "    if outdir is None:\n",
        "        outdir = NSDIR\n",
        "    outdir = Path(outdir)\n",
        "\n",
        "    # make database and load feature names; annoyingly slow\n",
        "    dataset = ns.Dataset(str(database))\n",
        "    dataset.add_features(str(features))\n",
        "    features = set(dataset.get_feature_names())\n",
        "\n",
        "    # if we only want a subset of the features take the set intersection\n",
        "    if use_features is not None:\n",
        "        features = set(features) & set(use_features)\n",
        "    pad = max([len(f) for f in features])\n",
        "\n",
        "    generated = []\n",
        "    for word in sorted(features):\n",
        "        msg = f'Running meta-analysis for term: {word:<{pad}}'\n",
        "        print(msg, end='\\r', flush=True)\n",
        "\n",
        "        # run meta-analysis + save specified outputs (only if they don't exist)\n",
        "        path = outdir / word.replace(' ', '_')\n",
        "        path.mkdir(exist_ok=True)\n",
        "        if not all((path / f'{f}.nii.gz').exists() for f in IMAGES):\n",
        "            ma = ns.MetaAnalysis(dataset, dataset.get_studies(features=word))\n",
        "            ma.save_results(path, image_list=IMAGES)\n",
        "\n",
        "        # store MA path\n",
        "        generated.append(path)\n",
        "\n",
        "    print(' ' * len(msg) + '\\b' * len(msg), end='', flush=True)\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "def parcellate_meta(outputs, annots, fname, regions):\n",
        "    # empty dataframe to hold our parcellated data\n",
        "    data = pd.DataFrame(index=regions)\n",
        "    mask = NiftiLabelsMasker(annots, resampling_target='data')\n",
        "\n",
        "    for outdir in outputs:\n",
        "        cdata = []\n",
        "        mgh = outdir / 'association-test_z.nii.gz'\n",
        "\n",
        "        cdata.append(mask.fit_transform(\n",
        "            check_niimg(mgh.__str__(), atleast_4d=True)).squeeze())\n",
        "\n",
        "        # aaaand store it in the dataframe\n",
        "        data = data.assign(**{outdir.name: np.hstack(cdata)})\n",
        "\n",
        "    # now we save the dataframe! wooo data!\n",
        "    data.to_csv(fname, sep=',')\n",
        "    return fname"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_07NHNTDVQwf"
      },
      "source": [
        "Now let's actually run the thing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dPGo3p-a4Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2ae70b-ae1a-4951-b0e2-b7d1e66b6e06"
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    NSDIR.mkdir(parents=True, exist_ok=True)\n",
        "    PARDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # get concepts from CogAtlas and run relevant NS meta-analysess,\n",
        "    database, features = fetch_ns_data(NSDIR)\n",
        "    generated = run_meta_analyses(database, features, get_cogatlas_concepts(),\n",
        "                                  outdir=NSDIR)\n",
        "\n",
        "    # get parcellations that we'll use to parcellate data\n",
        "    schaefer = fetch_atlas_schaefer_2018(n_rois=200, resolution_mm=2)\n",
        "    labels = []\n",
        "    for i in range(len(schaefer['labels'])):\n",
        "        labels.append(schaefer['labels'][i].decode(\"utf-8\"))\n",
        "\n",
        "    # parcellate data and save to directory\n",
        "    parcellate_meta(generated, schaefer['maps'],\n",
        "                    PARDIR / 'atl-schaefer2018_res-200_neurosynth.csv',\n",
        "                    regions=labels)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K26zqsmFVSwE"
      },
      "source": [
        "Download the parcellated `.csv` to upload back into your drive.\n",
        "Note you can also download the raw volumetric images from `./data/raw/neurosynth/term-of-interest/association-test_z.nii.gz`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8hNdmwc8T_Hi",
        "outputId": "2a246d1c-8bf3-4752-c885-b2a1973a2483"
      },
      "source": [
        "files.download(PARDIR / 'atl-schaefer2018_res-200_neurosynth.csv')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_54d99376-eeec-4faf-b598-56f3d516f122\", \"atl-schaefer2018_res-200_neurosynth.csv\", 494770)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}