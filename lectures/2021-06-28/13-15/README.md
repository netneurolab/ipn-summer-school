# Welcome to the dimensionality reduction lecture!



## Course materials

This is an introductory talk on dimensionality reduction including an overview of the methods and a coding demonstration session using Python. No prior knowledge on dimension reduction or machine learning is needed, however, we do assume some elementary understanding of linear algebra and programming.

The slides and demo notebooks are available in this repo and will be updated through the course. The notebooks can be ran either locally or online using Google Colab.


| Title        | Google Colab | Nbviewer |
| ------------- |:-------------:|:-----:|
| Demo 1: PCA & FA | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/netneurolab/ipn-summer-school/blob/main/lectures/2021-06-28/13-15/demo1.ipynb) | [![View the notebook](https://img.shields.io/badge/render-nbviewer-orange.svg)](https://nbviewer.jupyter.org/github/netneurolab/ipn-summer-school/blob/main/lectures/2021-06-28/13-15/demo1.ipynb?flush_cache=false) |
| Demo 2: Spectral embedding & tSNE | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/netneurolab/ipn-summer-school/blob/main/lectures/2021-06-28/13-15/demo2.ipynb) | [![View the notebook](https://img.shields.io/badge/render-nbviewer-orange.svg)](https://nbviewer.jupyter.org/github/netneurolab/ipn-summer-school/blob/main/lectures/2021-06-28/13-15/demo2.ipynb?flush_cache=false) |
| Demo 3: tSNE & UMAP | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/netneurolab/ipn-summer-school/blob/main/lectures/2021-06-28/13-15/demo3.ipynb) | [![View the notebook](https://img.shields.io/badge/render-nbviewer-orange.svg)](https://nbviewer.jupyter.org/github/netneurolab/ipn-summer-school/blob/main/lectures/2021-06-28/13-15/demo3.ipynb?flush_cache=false) |

## Resources

Here are some interesting papers or tools for your reference (not required for the lecture). 

**Classics**

- [van der Maaten et al. 2008](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.112.5472 ) *Dimensionality Reduction: A Comparative Review*
- [Cunningham and Yu, 2014](https://www.nature.com/articles/nn.3776) *Dimensionality reduction for large-scale neural recordings*
- [Coifman and Lafon, 2006](https://www.sciencedirect.com/science/article/pii/S1063520306000546) *Diffusion maps*
- [van der Maaten and Hinton, 2008](https://www.jmlr.org/papers/v9/vandermaaten08a.html) *Visualizing Data using t-SNE*
- [McInnes et al. 2018](https://arxiv.org/abs/1802.03426) *UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction*

**Some reviews**

- [Ahlheim and Love, 2018](https://www.sciencedirect.com/science/article/pii/S1053811918305226) *Estimating the functional dimensionality of neural representations*
- [Goddard et al. 2018](https://www.sciencedirect.com/science/article/pii/S1053811917305396) *Interpreting the dimensions of neural feature representations revealed by dimensionality reduction*
- [Bo et al. 2019](https://www.sciencedirect.com/science/article/pii/S0925231218312645) *A survey on Laplacian eigenmaps based manifold learning methods*
- [Williamson et al. 2019](https://www.sciencedirect.com/science/article/pii/S095943881830134X) *Bridging large-scale neuronal recordings and large-scale network models using dimensionality reduction*
- [Kobak and Berens, 2019](https://www.nature.com/articles/s41467-019-13056-x) *The art of using t-SNE for single-cell transcriptomics*


**Recent updates**

- [Sainburg et al. 2020](https://arxiv.org/abs/2009.12981) *Parametric UMAP embeddings for representation and semi-supervised learning*
- [Kobak and Linderman, 2021](https://www.nature.com/articles/s41587-020-00809-z) *Initialization is critical for preserving global data structure in both t-SNE and UMAP*
- [Hurwitz et al. 2021](https://arxiv.org/abs/2102.01807) *Building population models for large-scale neural recordings: opportunities and pitfalls*
- [Humphries 2021](https://arxiv.org/abs/2011.08088) *Strong and weak principles of neural dimension reduction*
- [Grace Lindsay's twitter thread on "dimensionality reduction designed for/by neuroscientists"](https://twitter.com/neurograce/status/1408431076075245570)


**Other interesting articles**

- [Wattenberg et al. 2016](https://distill.pub/2016/misread-tsne/) *How to Use t-SNE Effectively*
- [Heusser et al. 2017](https://arxiv.org/abs/1701.08290) *HyperTools: A Python toolbox for visualizing and manipulating high-dimensional data*
- [Nummenmaa et al. 2018](https://www.pnas.org/content/115/37/9198) *Maps of subjective feelings*
- [Li et al. 2020](https://distill.pub/2020/grand-tour/) *Visualizing Neural Networks with the Grand Tour*
- [Baron and Menard, 2020](https://arxiv.org/abs/2006.13948) *Extracting the main trend in a dataset: the Sequencer algorithm*
- [Deep Scatterplots](http://creatingdata.us/techne/deep_scatterplots/)
- [Li and Scheidegger](https://tiga1231.github.io/umap-tour/) *Toward Comparing DNNs with UMAP Tour*
- [Christopher Olah's blog](https://colah.github.io/)
