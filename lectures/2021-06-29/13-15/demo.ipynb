{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep our notebook clean, so it's a little more readable!\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate only if running from Google Collab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# %cd /content/gdrive/MyDrive/\n",
    "#\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using machine learning to predict age from rs-fMRI\n",
    "\n",
    "The goal is to extract data from several rs-fmri images, and use that data as features in a machine-learning model. We will integrate what we've learned in the previous machine-learning lecture to build an unbiased model and test it on a left out sample.\n",
    "\n",
    "![feat_xtrct](https://ars.els-cdn.com/content/image/1-s2.0-S1053811919301594-gr1.jpg)\n",
    "\n",
    "Link to slides: https://github.com/netneurolab/ipn-summer-school/blob/main/lectures/2021-06-29/13-15/20210629_ipn-summerschool_presentation_IntroML.pdf\n",
    "\n",
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "wdir = '/Users/estefanysuarez/nilearn_data/development_fmri'\n",
    "data = datasets.fetch_development_fmri(n_subjects=2, reduce_confounds=True, data_dir=None, resume=True, verbose=1, age_group='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do our machine learning, we will need to extract features from our rs-fmri images.\n",
    "\n",
    "Specifically, we will extract signals from a brain parcellation and compute a correlation matrix, representing regional coactivation between regions.\n",
    "\n",
    "We will practice on one subject first, then we'll extract data for all subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieve the atlas for extracting features and an example subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using rs-fmri data, it makes sense to use an atlas defined using rs-fmri data\n",
    "\n",
    "This paper has many excellent insights about what kind of atlas to use for an rs-fMRI machine learning task. See in particular Figure 5.\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1053811919301594?via%3Dihub\n",
    "\n",
    "Let's use the MIST atlas, created here in Montreal using the BASC method. This atlas has multiple resolutions, for larger networks or finer-grained ROIs. Let's use a 64-ROI atlas to allow some detail, but to ultimately keep our connectivity matrices manageable.\n",
    "\n",
    "Here is a link to the MIST paper: https://mniopenresearch.org/articles/1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "parcellations = datasets.fetch_atlas_basc_multiscale_2015(version='sym')\n",
    "atlas_filename = parcellations.scale064\n",
    "\n",
    "print('Atlas ROIs are located in nifti image (4D) at: %s' %\n",
    "       atlas_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at that atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "plotting.plot_roi(atlas_filename, draw_cross=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's load an example 4D fmri time-series for one subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_filenames = data.func[0]\n",
    "print(fmri_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the image! Because it is a 4D image, we can only look at one slice at a time. Or, better yet, let's look at an average image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image \n",
    "\n",
    "averaged_Img = image.mean_img(image.mean_img(fmri_filenames))\n",
    "plotting.plot_stat_map(averaged_Img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.view_img(averaged_Img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract signals on a parcellation defined by labels\n",
    "\n",
    "To do so, we are going to use the NiftiLabelsMasker. \n",
    "\n",
    "So we've loaded our atlas and 4D data for a single subject. Let's practice extracting features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True, \n",
    "                           memory='nilearn_cache', verbose=1)\n",
    "\n",
    "# Here we go from nifti files to the signal time series in a numpy\n",
    "# array. Note how we give confounds to be regressed out during signal\n",
    "# extraction\n",
    "conf = data.confounds[0]\n",
    "time_series = masker.fit_transform(fmri_filenames, confounds=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(time_series))\n",
    "print(time_series.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these \"confounds\" and how are they used? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "conf_df = pandas.read_table(conf)\n",
    "conf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute and display a correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "correlation_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Mask the main diagonal for visualization:\n",
    "np.fill_diagonal(correlation_matrix, 0)\n",
    "\n",
    "# The labels we have start with the background (0), hence we skip the\n",
    "# first label\n",
    "plotting.plot_matrix(correlation_matrix, figure=(10, 8), \n",
    "                     labels=range(time_series.shape[-1]),\n",
    "                     vmax=0.8, vmin=-0.8, reorder=False)\n",
    "\n",
    "# matrices are ordered for block-like representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract features from the whole dataset\n",
    "\n",
    "We can now use a for loop to iterate through each 4D image and use the same techniques we learned above to extract rs-fMRI connectivity features from every subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline\n",
    "\n",
    "## 1. Load the data\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/SampFeat.png\" alt=\"terms\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input features - X matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feat_file = 'MAIN_BASC064_subsamp_features.npz'\n",
    "X_features = np.load(feat_file)['a']\n",
    "\n",
    "n_samples, n_features = X_features.shape\n",
    "print(f'Our data set contains {n_samples} samples and {n_features} features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our features, we can visualize them as a matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X_features, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('feature matrix')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('subjects')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load output labels (our target data) - Y vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "# Let's load the phenotype data\n",
    "pheno_path = os.path.join('participants.tsv')\n",
    "\n",
    "pheno = pandas.read_csv(pheno_path, sep='\\t').sort_values('participant_id')\n",
    "pheno.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a column labeling age. Let's capture it in a variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_age = pheno['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should have a look at the distribution of our target variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.displot(y_age)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training/test data split\n",
    "\n",
    "Here, we will divide our data set into *training* and *test* sets. We will use the training set to fit and tweak our model's parameters. The test set will be exclusively used to validate our model, and therefore it will be set aside and **WE WILL NOT TOUCH IT UNTIL THE VERY END!!!**. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/splitdata.png\" alt=\"terms\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Now, we want to be sure that our training and test samples are matched! \n",
    "We can do that with a \"stratified split\". This dataset has a variable indicating AgeGroup. We can use that to make sure our training and testing sets are balanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_class = pheno['AgeGroup']\n",
    "age_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the sample into training/test sets with a 60/40 ratio, and \n",
    "# stratify by age class, and also shuffle the data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                    X_features, # x\n",
    "                                                    y_age, # y\n",
    "                                                    test_size = 0.4, # 60%/40% split  \n",
    "                                                    shuffle = True, # shuffle dataset\n",
    "                                                                    # before splitting\n",
    "                                                    stratify = age_class,  # keep\n",
    "                                                                           # distribution\n",
    "                                                                           # of ageclass\n",
    "                                                                           # consistent\n",
    "                                                                           # betw. train\n",
    "                                                                           # & test sets.\n",
    "                                                    random_state = 123 # same shuffle each\n",
    "                                                                       # time\n",
    "                                                                       )\n",
    "\n",
    "# print the size of our training and test groups\n",
    "print('Training sample:', len(X_train))\n",
    "print('Test sample:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distributions to be sure they are matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_train, label='train', stat='density', kde=True, color=sns.color_palette()[0])\n",
    "sns.histplot(y_test, label='test', stat='density', kde=True, color=sns.color_palette()[1])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train your first ML model!\n",
    "\n",
    "Machine-learning can get fancy pretty quickly. We'll start with a fairly standard regression model called a **Support Vector Regressor (SVR)**. \n",
    "\n",
    "While this may seem unambitious, simple models can be very robust. And we probably don't have enough data to create more complex models (but we can try later).\n",
    "\n",
    "For more information, see this excellent resource:\n",
    "https://hal.inria.fr/hal-01824205\n",
    "\n",
    "Let's fit our first model!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/modfit.png\" alt=\"terms\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# define the model\n",
    "l_svr = SVR(kernel='linear') \n",
    "\n",
    "# fit the model\n",
    "l_svr.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate your model\n",
    "Well... that was easy. Now let's see how well the model learned the data!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/modval.png\" alt=\"terms\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "But, two questions first:\n",
    "\n",
    "1. What happens if I validate my model with my **train** set? \n",
    "2. What happens if I validate my model with the **test** set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with the **training** set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the training data based on the model\n",
    "y_pred = l_svr.predict(X_train) \n",
    "\n",
    "# caluclate the model accuracy\n",
    "acc = l_svr.score(X_train, y_train) # the default score for the SVR model is the coefficient of determination R2\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_true=y_train,y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view our results and plot them all at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "print('Accuracy (R2) = ', acc) \n",
    "print('MAE           = ', mae) \n",
    "\n",
    "sns.regplot(y_pred,y_train)\n",
    "plt.xlabel('Predicted Age')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOLY COW! Machine-learning is amazing!!! Almost a perfect fit :O ... which means there's must be something wrong. What's the problem here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation (CV)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/CV.png\" alt=\"terms\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the sample into training/validation sets with a 75/25 ratio, and \n",
    "# stratify by age class, and also shuffle the data.\n",
    "\n",
    "age_class2 = pheno.loc[y_train.index,'AgeGroup']\n",
    "\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(\n",
    "                                                    X_train, # x\n",
    "                                                    y_train, # y\n",
    "                                                    test_size = 0.25, # 75%/25% split  \n",
    "                                                    shuffle = True, # shuffle dataset\n",
    "                                                                    # before splitting\n",
    "                                                    stratify = age_class2,  # keep\n",
    "                                                                           # distribution\n",
    "                                                                           # of ageclass\n",
    "                                                                           # consistent\n",
    "                                                                           # betw. train\n",
    "                                                                           # & test sets.\n",
    "                                                    random_state = 123 # same shuffle each\n",
    "                                                                       # time\n",
    "                                                                       )\n",
    "\n",
    "# print the size of our new training and validation groups\n",
    "print('(New) Training sample:', len(X_train2))\n",
    "print('Validation sample:', len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# fit model just to training data\n",
    "l_svr.fit(X_train2,y_train2)\n",
    "\n",
    "# predict the *test* data based on the model trained on X_train2\n",
    "y_pred = l_svr.predict(X_val) \n",
    "\n",
    "# caluclate the model accuracy\n",
    "acc = l_svr.score(X_val, y_val) \n",
    "mae = mean_absolute_error(y_true=y_val,y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "print('CV accuracy (R2) = ', acc)\n",
    "print('CV MAE           = ', mae)\n",
    "\n",
    "sns.regplot(y_pred,y_val)\n",
    "plt.xlabel('Predicted Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect, but as predicting with unseen data goes, not too bad! Especially with a training sample of \"only\" 69 subjects. But we can do better in terms of estimating the accuracy of our model's performance!\n",
    "\n",
    "For example, we can \"increase\" the size of our training set by using 10-fold cross-validation (CV) instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using *k*-fold CV to accurately assess model performance\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/KCV1.png\" alt=\"terms\" width=\"900\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "# estimate CV scores\n",
    "n_folds = 5\n",
    "acc = cross_val_score(l_svr, X_train, y_train, cv=n_folds)\n",
    "mae = cross_val_score(l_svr, X_train, y_train, cv=n_folds, scoring='neg_mean_absolute_error')\n",
    "print(f\"average ({n_folds}-fold) CV accuracy (R2) = %0.2f and standard deviation of %0.2f\" % (acc.mean(), acc.std()))\n",
    "print(f\"average ({n_folds}-fold) CV MAE           = %0.2f and standard deviation of %0.2f\" % (mae.mean(), mae.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the accuracy of the predictions for each fold of the cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_folds):\n",
    "    print('Fold {} -- Acc = {}, MAE = {}'.format(i, acc[i], mae[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can also look at the overall accuracy of the model. In this case the data is split according to the CV parameter. Each sample belongs to exactly one validation set, and its prediction is computed with the model fitted on the corresponding training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = cross_val_predict(l_svr, X_train, y_train, cv=n_folds) # here we use our original training set\n",
    "overall_acc = r2_score(y_train, y_pred)\n",
    "overall_mae = mean_absolute_error(y_train,y_pred)\n",
    "\n",
    "print('Overall CV R2:', overall_acc)\n",
    "print('Overall CV MAE:', overall_mae)\n",
    "\n",
    "sns.regplot(y_pred, y_train)\n",
    "plt.xlabel('Predicted Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tweak your model: *k*-fold CV for model selection\n",
    "\n",
    "It's very important to learn when and where its appropriate to \"tweak\" your model.\n",
    "\n",
    "Since we have done all of the previous analysis in our training data, it's fine to try out different models. But we **absolutely cannot** \"test\" it on our left out data. If we do, we are in great danger of overfitting.\n",
    "\n",
    "It is not uncommon to try other models, or tweak hyperparameters. In this case, due to our relatively small sample size, we are probably not powered sufficiently to do so, and we would once again risk overfitting. However, for the sake of demonstration, we will do some tweaking. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/KCV2.png\" alt=\"terms\" width=\"400\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# define models and set hyperparameters\n",
    "svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n",
    "svr_rbf = SVR(kernel='rbf', C=100, gamma='auto') #0.1, epsilon=.1)\n",
    "svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=2) #, epsilon=.1, coef0=1)\n",
    "\n",
    "models = {'svr_lin':svr_lin, 'svr_rbf':svr_rbf, 'svr_poly':svr_poly}\n",
    "\n",
    "# perform k-fold CV with each model and evaluate performance\n",
    "n_folds = 5\n",
    "for name, model in models.items():\n",
    "    print(f'\\nPerformance for the {name.upper()} model:')\n",
    "    acc = cross_val_score(model, X_train, y_train, cv=n_folds)\n",
    "    mae = cross_val_score(model, X_train, y_train, cv=n_folds, scoring='neg_mean_absolute_error')\n",
    "    print(f\" average ({n_folds}-fold) CV accuracy (R2) = %0.2f with a standard deviation of %0.2f\" % (acc.mean(), acc.std()))\n",
    "    print(f\" average ({n_folds}-fold) CV MAE           = %0.2f with a standard deviation of %0.2f\" % (mae.mean(), mae.std()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to our k-fold CV analyses, we can identify the *best* candidate model. Note however that the SVR model possesses a number of *hyperparameters* that could be tuned to potentially improve our predictions. The type of kernel is a hyperparameter itself. Let's try this! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tweak your model: *k*-fold CV + Grid Search for hyperparameter tuning\n",
    "\n",
    "To tune our model hyperparameters we can combine grid searh and cross validation!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/KCV3.png\" alt=\"terms\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "SVR model documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# let's create our grid search \n",
    "tunning_params = [{'kernel': ['poly', 'linear', 'rbf'], 'gamma': [1e-3, 1e-4, 'scale', 'auto'],\n",
    "                   'C': [1, 10, 100, 1000], 'degree':[2,3,4,5], 'epsilon':[.1], 'coef0':[1]}]\n",
    "\n",
    "print(\"Tuning hyper-parameters ... \")\n",
    "n_folds = 5\n",
    "clf = GridSearchCV(estimator=SVR(), param_grid=tunning_params, cv=n_folds)#, scoring='neg_mean_absolute_error')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nGrid scores on development set:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "print(f\"\\nBest score (R2): {clf.best_score_}\")\n",
    "# print(f\"\\nBest score (MAE): {clf.best_score_}\")\n",
    "print(\"Best parameters set found on development set:\")\n",
    "opt_hyperparams = clf.best_params_\n",
    "print(opt_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to our combined *k*-fold CV + Grid Search analysis, we have found the *best* model and the *best* set of model hyperparameters that maximize our (cross-validated) accuracy score, now what? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.  Ensemble methods and the *bias-variance* trade-off \n",
    "\n",
    "Now that we have selected the model and its hyper-parameters, we can now use ensemble methods, such as **bagging** and **boosting**, to improve the bias-variance trade-off of our model.  \n",
    "\n",
    "In general, ensemble methods provide a way to reduce overfitting. **Bagging** improves the trade-off between bias and variance by slightly increasing the bias term, but allowing for a larger reduction of the variance term. Thefore, **bagging** methods are better suited for **strong and complex** models in which the variance is high.\n",
    "\n",
    "On the other hand, **Boosting** improves the trade-off between bias and variance by slightly increasing the variance term, but allowing for a larger reduction of the bias term. Hence, **boosting** methods usually work best with **weak and simple** models in which the bias is high. \n",
    "\n",
    "These slight changes in the bias and/or the variance term *generally* result in a lower overall error and thus a better performance. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"Imgs/ensemble.png\" alt=\"terms\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "estimators = [(\"SVR\", SVR(**opt_hyperparams)),\n",
    "              (\"Bagging(SVR)\", BaggingRegressor(SVR(**opt_hyperparams))),\n",
    "              (\"Boosting(SVR)\", AdaBoostRegressor(SVR(**opt_hyperparams)))\n",
    "             ]\n",
    "\n",
    "# Loop over estimators to compare\n",
    "n_folds = 5\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    print(f'\\nPerformance for the {name.upper()} model:')\n",
    "    acc = cross_val_score(estimator, X_train, y_train, cv=n_folds)\n",
    "    mae = cross_val_score(estimator, X_train, y_train, cv=n_folds, scoring='neg_mean_absolute_error')\n",
    "    print(f\" average ({n_folds}-fold) CV accuracy (R2) = %0.2f with a standard deviation of %0.2f\" % (acc.mean(), acc.std()))\n",
    "    print(f\" average ({n_folds}-fold) CV MAE           = %0.2f with a standard deviation of %0.2f\" % (mae.mean(), mae.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 4\n",
    "n_val = 13\n",
    "\n",
    "age_class2 = pheno.loc[y_train.index,'AgeGroup']\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(\n",
    "                                                    X_train, \n",
    "                                                    y_train, \n",
    "                                                    test_size = n_val,  \n",
    "                                                    shuffle = True, \n",
    "                                                    stratify = age_class2,\n",
    "                                                    )\n",
    "\n",
    "X_train2 = np.split(X_train2, n_repeat)\n",
    "y_train2 = np.split(y_train2, n_repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"SVR\", SVR(**opt_hyperparams)),\n",
    "              (\"Bagging(SVR)\", BaggingRegressor(SVR(**opt_hyperparams))),\n",
    "              (\"Boosting(SVR)\", AdaBoostRegressor(SVR(**opt_hyperparams)))\n",
    "             ]\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    \n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros((n_val, n_repeat))\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        estimator.fit(X_train2[i], y_train2[i])\n",
    "        y_predict[:, i] = estimator.predict(X_val)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_val)\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        y_error += (y_val - y_predict[:, i]) ** 2\n",
    "\n",
    "    y_error /= (n_repeat * n_repeat)\n",
    "    y_bias = (y_val - np.mean(y_predict, axis=1)) ** 2\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "    \n",
    "    print(\"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n",
    "          \" + {3:.4f} (var)\".format(name,\n",
    "                                    np.mean(y_error),\n",
    "                                    np.mean(y_bias),\n",
    "                                    np.mean(y_var),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neither ensemble method yields better performance compared to the single estimator, notice how **boosting** contributes to reduce the bias of the model, whereas **bagging** contributes to reduce its variance!!! \n",
    "\n",
    "Which method is then the best? It really depends on the application. Some applications require a higher accuracy (low bias), whereas other prefer to sacrifice a bit of accuracy while guaranteeing a higher precision (low variance). There is always a trade-off!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.  Model deployment\n",
    "\n",
    "Ok, so now that we have found the optimal model and its hyperparameters its time to release a working version of the model for \"production\". This process implies at least two things:\n",
    "\n",
    "* **Report model performance**\n",
    "* **Prepare model for production**  \n",
    "\n",
    "Because we are not planning to make further changes in the model, we can now use our entire data set to report our model's performance, and to train our final model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = SVR(**opt_hyperparams)\n",
    "n_folds = 5\n",
    "acc = cross_val_score(estimator, X_features, y_age, cv=n_folds)\n",
    "mae = cross_val_score(estimator, X_features, y_age, cv=n_folds, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"Model: SVR\")\n",
    "print(f\"Model hyperparameters:{opt_hyperparams}\")\n",
    "\n",
    "print(f\"\\nModel performance:\")\n",
    "print(f\" average ({n_folds}-fold) CV accuracy (R2) = %0.2f with a standard deviation of %0.2f\" % (acc.mean(), acc.std()))\n",
    "print(f\" average ({n_folds}-fold) CV MAE           = %0.2f with a standard deviation of %0.2f\" % (mae.mean(), mae.std()))\n",
    "\n",
    "print(f\"\\nModel performance per fold:\")\n",
    "for i in range(n_folds):\n",
    "    print(' Fold {} -- Acc = {}, MAE = {}'.format(i, acc[i], mae[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_features,y_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "1. The MORE DATA the better!\n",
    "2. BUT ... hopefully your data is as heterogeneous as possible to avoid BIASES in the model! \n",
    "3. ALWAYS get familiar with your data first --> use visualization tools! \n",
    "4. Do NOT touch your test data set until the very END!\n",
    "5. Always go from simple to complex, especially if you do not have a lot observations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
